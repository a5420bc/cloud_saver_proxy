from ..base import BaseSearch
import requests
import re
from bs4 import BeautifulSoup
import concurrent.futures
import random
import time
import logging
from urllib.parse import urljoin, urlparse
from requests.adapters import HTTPAdapter

# å¸¸é‡å®šä¹‰
BASE_URL = "https://4kfox.com"
SEARCH_URL = BASE_URL + "/search/%s-------------.html"
SEARCH_PAGE_URL = BASE_URL + "/search/%s----------%d---.html"
DETAIL_URL = BASE_URL + "/video/%s.html"
DEFAULT_TIMEOUT = 15  # é»˜è®¤è¶…æ—¶æ—¶é—´ - å¢åŠ è¶…æ—¶æ—¶é—´é¿å…ç½‘ç»œæ…¢çš„é—®é¢˜
DEFAULT_HTTP_PROXY = "http://154.219.110.34:51422"
DEFAULT_SOCKS5_PROXY = "socks5://154.219.110.34:51423"
DEBUG_MODE = False  # è°ƒè¯•å¼€å…³ - é»˜è®¤å…³é—­
PROXY_ENABLED = False  # ä»£ç†å¼€å…³ - é»˜è®¤å…³é—­
MAX_CONCURRENCY = 50  # å¹¶å‘æ•°é™åˆ¶ - å¤§å¹…æé«˜å¹¶å‘æ•°
MAX_PAGES = 10  # æœ€å¤§åˆ†é¡µæ•°ï¼ˆé¿å…æ— é™è¯·æ±‚ï¼‰

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
DETAIL_ID_REGEX = re.compile(r'/video/(\d+)\.html')
MAGNET_LINK_REGEX = re.compile(r'magnet:\?xt=urn:btih:[0-9a-fA-F]{40}[^"\'\s]*')
ED2K_LINK_REGEX = re.compile(r'ed2k://\|file\|[^|]+\|[^|]+\|[^|]+\|/?')
YEAR_REGEX = re.compile(r'(\d{4})')
PAN_LINK_REGEXES = {
    "baidu": re.compile(r'https?://pan\.baidu\.com/s/[0-9a-zA-Z_-]+(?:\?pwd=[0-9a-zA-Z]+)?(?:&v=\d+)?'),
    "aliyun": re.compile(r'https?://(?:www\.)?alipan\.com/s/[0-9a-zA-Z_-]+'),
    "tianyi": re.compile(r'https?://cloud\.189\.cn/t/[0-9a-zA-Z_-]+(?:\([^)]*\))?'),
    "uc": re.compile(r'https?://drive\.uc\.cn/s/[0-9a-fA-F]+(?:\?[^"\s]*)?'),
    "mobile": re.compile(r'https?://caiyun\.139\.com/[^"\s]+'),
    "115": re.compile(r'https?://115\.com/s/[0-9a-zA-Z_-]+'),
    "pikpak": re.compile(r'https?://mypikpak\.com/s/[0-9a-zA-Z_-]+'),
    "xunlei": re.compile(r'https?://pan\.xunlei\.com/s/[0-9a-zA-Z_-]+(?:\?pwd=[0-9a-zA-Z]+)?'),
    "123": re.compile(r'https?://(?:www\.)?123pan\.com/s/[0-9a-zA-Z_-]+'),
    "quark": re.compile(r'https?://pan\.quark\.cn/s/[0-9a-fA-F]+(?:\?pwd=[0-9a-zA-Z]+)?'),
}
QUARK_LINK_REGEX = re.compile(r'https?://pan\.quark\.cn/s/[0-9a-fA-F]+(?:\?pwd=[0-9a-zA-Z]+)?')
PASSWORD_REGEXES = [
    re.compile(r'\?pwd=([0-9a-zA-Z]+)'),  # URLä¸­çš„pwdå‚æ•°
    re.compile(r'æå–ç [ï¼š:]\s*([0-9a-zA-Z]+)'),  # æå–ç ï¼šxxxx
    re.compile(r'è®¿é—®ç [ï¼š:]\s*([0-9a-zA-Z]+)'),  # è®¿é—®ç ï¼šxxxx
    re.compile(r'å¯†ç [ï¼š:]\s*([0-9a-zA-Z]+)'),  # å¯†ç ï¼šxxxx
    re.compile(r'ï¼ˆè®¿é—®ç [ï¼š:]\s*([0-9a-zA-Z]+)ï¼‰'),  # ï¼ˆè®¿é—®ç ï¼šxxxxï¼‰
]

# æ€§èƒ½ç»Ÿè®¡ï¼ˆåŸå­æ“ä½œï¼‰
search_requests = 0
detail_page_requests = 0
cache_hits = 0
cache_misses = 0
total_search_time = 0  # çº³ç§’
total_detail_time = 0  # çº³ç§’

class Fox4kSearch(BaseSearch):
    def __init__(self):
        self.optimized_client = self.create_optimized_http_client()

    def create_proxy_transport(self, proxy_url):
        transport = {
            'max_idle_conns': 200,
            'max_idle_conns_per_host': 50,
            'max_conns_per_host': 100,
            'idle_conn_timeout': 90,
            'disable_keep_alives': False,
            'disable_compression': False,
            'write_buffer_size': 16 * 1024,
            'read_buffer_size': 16 * 1024,
        }

        if not proxy_url:
            return transport

        if proxy_url.startswith("socks5://"):
            # SOCKS5ä»£ç†
            transport['proxy'] = {'http': proxy_url, 'https': proxy_url}
            if DEBUG_MODE:
                logging.debug(f"ğŸ”§ [Fox4k DEBUG] ä½¿ç”¨SOCKS5ä»£ç†: {proxy_url}")
        else:
            # HTTPä»£ç†
            transport['proxy'] = {'http': proxy_url, 'https': proxy_url}
            if DEBUG_MODE:
                logging.debug(f"ğŸ”§ [Fox4k DEBUG] ä½¿ç”¨HTTPä»£ç†: {proxy_url}")

        return transport

    def create_optimized_http_client(self):
        selected_proxy = ""

        if PROXY_ENABLED:
            # éšæœºé€‰æ‹©ä»£ç†ç±»å‹
            proxy_types = ["", DEFAULT_HTTP_PROXY, DEFAULT_SOCKS5_PROXY]
            selected_proxy = random.choice(proxy_types)
        else:
            # ä»£ç†æœªå¯ç”¨ï¼Œä½¿ç”¨ç›´è¿
            selected_proxy = ""
            if DEBUG_MODE:
                logging.debug("ğŸ”§ [Fox4k DEBUG] ä»£ç†åŠŸèƒ½å·²ç¦ç”¨ï¼Œä½¿ç”¨ç›´è¿æ¨¡å¼")

        transport = self.create_proxy_transport(selected_proxy)
        if not transport:
            transport = self.create_proxy_transport("")

        if not selected_proxy and PROXY_ENABLED:
            logging.debug("ğŸ”§ [Fox4k DEBUG] ä½¿ç”¨ç›´è¿æ¨¡å¼")
        session = requests.Session()
                # é…ç½®é€‚é…å™¨ï¼Œå¢åŠ è¿æ¥æ± å¤§å°
        adapter = HTTPAdapter(
            pool_connections=20,  # è¿æ¥æ± ä¸­çš„è¿æ¥æ•°
            pool_maxsize=50,      # è¿æ¥æ± æœ€å¤§è¿æ¥æ•°
        )
        
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session

    def search(self, keyword):
        result, err = self.search_with_result(keyword)
        if err:
            return None
        return result

    def search_with_result(self, keyword):
        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] SearchWithResult å¼€å§‹ - keyword: {keyword}")

        result, err = self.async_search_with_result(keyword, self.search_impl)
        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] SearchWithResult å®Œæˆ - ç»“æœæ•°: {len(result['results'])}, é”™è¯¯: {err}")
        if result['results']:
            logging.debug("ğŸ”§ [Fox4k DEBUG] å‰3ä¸ªç»“æœç¤ºä¾‹:")
            for i, r in enumerate(result['results']):
                if i >= 3:
                    break
                logging.debug(f"  {i+1}. æ ‡é¢˜: {r['title']}, é“¾æ¥æ•°: {len(r['links'])}")

        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        standard_results = []
        for item in result['results']:
            # è½¬æ¢é“¾æ¥æ ¼å¼
            cloud_links = []
            for link in item['links']:
                cloud_link = {
                    "link": link['url'] + (f"?pwd={link['password']}" if link['password'] else ""),
                    "cloudType": link['type']
                }
                cloud_links.append(cloud_link)

            standard_item = {
                "messageId": item['unique_id'],
                "title": item['title'],
                "pubDate": "2022-11-03T14:07:54+00:00",  # ä½¿ç”¨å›ºå®šæ—¶é—´
                "content": item['content'],
                "image": "",  # æš‚æ—¶ä¸ºç©ºï¼Œå¯ä»¥è€ƒè™‘ä»itemä¸­æå–å›¾ç‰‡URL
                "cloudLinks": cloud_links,
                "tags": item['tags'],
                "magnetLink": "",  # æš‚æ—¶ä¸ºç©º
                "channel": "4Kå½±è§†",
                "channelId": "fox4k"
            }
            standard_results.append(standard_item)

        return {
            "list": standard_results,
            "channelInfo": {
                "id": "fox4k",
                "name": "4Kå½±è§†",
                "index": 1000,
                "channelLogo": ""
            },
            "id": "fox4k",
            "index": 1000
        }, None

    def search_impl(self, keyword):
        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] searchImpl å¼€å§‹æ‰§è¡Œ - keyword: {keyword}")
        start_time = time.time()
        global search_requests
        search_requests += 1

        encoded_keyword = requests.utils.quote(keyword)
        all_results = []

        # 1. æœç´¢ç¬¬ä¸€é¡µï¼Œè·å–æ€»é¡µæ•°
        first_page_results, total_pages, err = self.search_page(encoded_keyword, 1)
        if err:
            return None, err
        all_results.extend(first_page_results)

        # 2. å¦‚æœæœ‰å¤šé¡µï¼Œç»§ç»­æœç´¢å…¶ä»–é¡µé¢ï¼ˆé™åˆ¶æœ€å¤§é¡µæ•°ï¼‰
        max_pages_to_search = total_pages
        if max_pages_to_search > MAX_PAGES:
            max_pages_to_search = MAX_PAGES

        if total_pages > 1 and max_pages_to_search > 1:
            # å¹¶å‘æœç´¢å…¶ä»–é¡µé¢
            results = [None] * (max_pages_to_search - 1)
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENCY) as executor:
                futures = {executor.submit(self.search_page, encoded_keyword, page): page for page in range(2, max_pages_to_search + 1)}
                for future in concurrent.futures.as_completed(futures):
                    page = futures[future]
                    page_results, _, err = future.result()
                    if not err:
                        all_results.extend(page_results)

        # 3. å¹¶å‘è·å–è¯¦æƒ…é¡µä¿¡æ¯
        all_results = self.enrich_with_detail_info(all_results)

        # 4. è¿‡æ»¤å…³é”®è¯åŒ¹é…çš„ç»“æœ
        results = self.filter_results_by_keyword(all_results, keyword)

        # è®°å½•æ€§èƒ½ç»Ÿè®¡
        search_duration = time.time() - start_time
        global total_search_time
        total_search_time += search_duration

        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] searchImpl å®Œæˆ - åŸå§‹ç»“æœ: {len(all_results)}, è¿‡æ»¤åç»“æœ: {len(results)}, è€—æ—¶: {search_duration}ç§’")

        return results, None

    def search_page(self, encoded_keyword, page):
        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] searchPage å¼€å§‹ - ç¬¬{page}é¡µ, keyword: {encoded_keyword}")

        # 1. æ„å»ºæœç´¢URL
        search_url = SEARCH_URL % encoded_keyword if page == 1 else SEARCH_PAGE_URL % (encoded_keyword, page)

        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] æ„å»ºçš„URL: {search_url}")

        # 2. åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡
        start_time = time.time()

        # 3. åˆ›å»ºè¯·æ±‚
        headers = {
            "User-Agent": self.get_random_ua(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Cache-Control": "max-age=0",
            "Referer": BASE_URL + "/",
            "X-Forwarded-For": self.generate_random_ip(),
            "X-Real-IP": self.generate_random_ip(),
            "sec-ch-ua-platform": "macOS",
        }

        if DEBUG_MODE:
            logging.debug("ğŸ”§ [Fox4k DEBUG] ä½¿ç”¨éšæœºUA: %s", headers["User-Agent"])
            logging.debug("ğŸ”§ [Fox4k DEBUG] ä½¿ç”¨éšæœºIP: %s", headers["X-Forwarded-For"])

        # 5. å‘é€HTTPè¯·æ±‚
        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] å¼€å§‹å‘é€HTTPè¯·æ±‚åˆ°: {search_url}")
            logging.debug("ğŸ”§ [Fox4k DEBUG] è¯·æ±‚å¤´ä¿¡æ¯:")
            for key, value in headers.items():
                logging.debug(f"    {key}: {value}")

        resp, err = self.do_request_with_retry(search_url, headers)
        if err:
            return None, 0, err

        # 6. æ£€æŸ¥çŠ¶æ€ç 
        if resp.status_code != 200:
            if DEBUG_MODE:
                logging.debug(f"âŒ [Fox4k DEBUG] çŠ¶æ€ç å¼‚å¸¸: {resp.status_code}")
            return None, 0, f"ç¬¬{page}é¡µè¯·æ±‚è¿”å›çŠ¶æ€ç : {resp.status_code}"

        # 7. è¯»å–å¹¶æ‰“å°HTMLå“åº”
        html_content = resp.text
        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] ç¬¬{page}é¡µ HTMLé•¿åº¦: {len(html_content)} bytes")

        # ä¿å­˜HTMLåˆ°æ–‡ä»¶ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼‰
        if DEBUG_MODE:
            html_dir = "./html"
            import os
            os.makedirs(html_dir, exist_ok=True)
            filename = f"fox4k_page_{page}_{encoded_keyword.replace('%', '_')}.html"
            filepath = os.path.join(html_dir, filename)
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                logging.debug(f"âœ… [Fox4k DEBUG] HTMLå·²ä¿å­˜åˆ°: {filepath}")
            except Exception as e:
                logging.debug(f"âŒ [Fox4k DEBUG] ä¿å­˜HTMLæ–‡ä»¶å¤±è´¥: {e}")

        # è§£æHTMLå“åº”
        doc = BeautifulSoup(html_content, 'html.parser')

        # 8. è§£æåˆ†é¡µä¿¡æ¯
        total_pages = self.parse_total_pages(doc)

        # 9. æå–æœç´¢ç»“æœ
        results = []
        for item in doc.select('.hl-list-item'):
            result = self.parse_search_result_item(item)
            if result:
                results.append(result)
        return results, total_pages, None

    def parse_total_pages(self, doc):
        # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯ï¼Œæ ¼å¼ä¸º "1 / 2"
        page_info = doc.select_one('.hl-page-tips a')
        if not page_info:
            return 1
        page_info_text = page_info.get_text(strip=True)
        if not page_info_text:
            return 1

        # è§£æ "1 / 2" æ ¼å¼
        parts = page_info_text.split('/')
        if len(parts) != 2:
            return 1

        total_pages_str = parts[1].strip()
        try:
            total_pages = int(total_pages_str)
            if total_pages < 1:
                return 1
            return total_pages
        except ValueError:
            return 1

    def parse_search_result_item(self, item):
        # è·å–è¯¦æƒ…é¡µé“¾æ¥
        link_element = item.select_one('.hl-item-pic a')
        if not link_element:
            return None
        href = link_element.get('href')
        if not href:
            return None

        # è¡¥å…¨URL
        if href.startswith('/'):
            href = urljoin(BASE_URL, href)

        # æå–ID
        matches = DETAIL_ID_REGEX.search(href)
        if not matches or len(matches.groups()) < 1:
            return None
        id = matches.group(1)

        # è·å–æ ‡é¢˜
        title_element = item.select_one('.hl-item-title a')
        if not title_element:
            return None
        title = title_element.get_text(strip=True)
        if not title:
            return None

        # è·å–å°é¢å›¾ç‰‡
        img_element = item.select_one('.hl-item-thumb')
        image_url = img_element.get('data-original', '') if img_element else ''
        if image_url and image_url.startswith('/'):
            image_url = urljoin(BASE_URL, image_url)

        # è·å–èµ„æºçŠ¶æ€
        status = item.select_one('.hl-pic-text .remarks')
        status_text = status.get_text(strip=True) if status else ''

        # è·å–è¯„åˆ†
        score = item.select_one('.hl-text-conch.score')
        score_text = score.get_text(strip=True) if score else ''

        # è·å–åŸºæœ¬ä¿¡æ¯ï¼ˆå¹´ä»½ã€åœ°åŒºã€ç±»å‹ï¼‰
        basic_info_elements = item.select('.hl-item-sub')
        basic_info = basic_info_elements[0].get_text(strip=True) if basic_info_elements else ''

        # è·å–ç®€ä»‹
        description = basic_info_elements[-1].get_text(strip=True) if basic_info_elements else ''

        # è§£æå¹´ä»½ã€åœ°åŒºã€ç±»å‹
        year, region, category = "", "", ""
        if basic_info:
            parts = basic_info.split('Â·')
            for part in parts:
                part = part.strip()
                if not part:
                    continue
                if score_text in part:
                    continue
                if not year and YEAR_REGEX.match(part):
                    year = part
                elif not region:
                    region = part
                elif not category:
                    category = part
                else:
                    category += " " + part

        # æ„å»ºæ ‡ç­¾
        tags = []
        if status_text:
            tags.append(status_text)
        if year:
            tags.append(year)
        if region:
            tags.append(region)
        if category:
            tags.append(category)

        # æ„å»ºå†…å®¹æè¿°
        content = description
        if basic_info:
            content = basic_info + "\n" + description
        if score_text:
            content = "è¯„åˆ†: " + score_text + "\n" + content

        return {
            "unique_id": f"fox4k-{id}",
            "title": title,
            "content": content,
            "datetime": None,  # ä½¿ç”¨é›¶å€¼è€Œä¸æ˜¯Noneï¼Œå‚è€ƒjikepanæ’ä»¶æ ‡å‡†
            "tags": tags,
            "links": [],  # åˆå§‹ä¸ºç©ºï¼Œåç»­åœ¨è¯¦æƒ…é¡µä¸­å¡«å……
            "channel": "",  # æ’ä»¶æœç´¢ç»“æœï¼ŒChannelå¿…é¡»ä¸ºç©º
        }

    def enrich_with_detail_info(self, results):
        if not results:
            return results

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = [True] * MAX_CONCURRENCY
        enriched_results = results.copy()

        def fetch_detail(index):
            nonlocal semaphore
            # è·å–è¯¦æƒ…é¡µä¿¡æ¯
            detail_info = self.get_detail_info(enriched_results[index]['unique_id'].split('-')[1])
            if detail_info:
                enriched_results[index]['links'] = detail_info['downloads']
                if detail_info['content']:
                    enriched_results[index]['content'] = detail_info['content']
                # è¡¥å……æ ‡ç­¾
                for tag in detail_info['tags']:
                    if tag not in enriched_results[index]['tags']:
                        enriched_results[index]['tags'].append(tag)

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENCY) as executor:
            futures = [executor.submit(fetch_detail, i) for i in range(len(enriched_results))]
            concurrent.futures.wait(futures)

        # è¿‡æ»¤æ‰æ²¡æœ‰æœ‰æ•ˆä¸‹è½½é“¾æ¥çš„ç»“æœ
        valid_results = [result for result in enriched_results if result['links']]
        return valid_results

    def get_detail_info(self, id):
        if DEBUG_MODE:
            logging.debug(f"ğŸ”§ [Fox4k DEBUG] getDetailInfo å¼€å§‹ - ID: {id}")
        start_time = time.time()
        global detail_page_requests
        detail_page_requests += 1

        # æ„å»ºè¯¦æƒ…é¡µURL
        detail_url = DETAIL_URL % id

        # åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Referer": BASE_URL + "/",
        }

        # å‘é€è¯·æ±‚
        resp, err = self.do_request_with_retry(detail_url, headers)
        if err:
            return None

        # è§£æHTML
        doc = BeautifulSoup(resp.text, 'html.parser')

        # è§£æè¯¦æƒ…é¡µä¿¡æ¯
        detail = {
            'downloads': [],
            'tags': [],
            'timestamp': time.time(),
        }

        # è·å–æ ‡é¢˜
        title_element = doc.select_one('h2.hl-dc-title')
        detail['title'] = title_element.get_text(strip=True) if title_element else ''

        # è·å–å°é¢å›¾ç‰‡
        img_element = doc.select_one('.hl-dc-pic .hl-item-thumb')
        if img_element and 'data-original' in img_element.attrs:
            image_url = img_element['data-original']
            if image_url.startswith('/'):
                image_url = urljoin(BASE_URL, image_url)
            detail['image_url'] = image_url

        # è·å–å‰§æƒ…ç®€ä»‹
        content_element = doc.select_one('.hl-content-wrap .hl-content-text')
        detail['content'] = content_element.get_text(strip=True) if content_element else ''

        # æå–è¯¦ç»†ä¿¡æ¯ä½œä¸ºæ ‡ç­¾
        for li in doc.select('.hl-vod-data ul li'):
            text = li.get_text(strip=True)
            if text:
                # æ¸…ç†æ ‡ç­¾æ–‡æœ¬
                text = text.replace("ï¼š", ": ")
                if "ç±»å‹:" in text or "åœ°åŒº:" in text or "è¯­è¨€:" in text:
                    detail['tags'].append(text)

        # æå–ä¸‹è½½é“¾æ¥
        self.extract_download_links(doc, detail)

        # è®°å½•æ€§èƒ½ç»Ÿè®¡
        detail_duration = time.time() - start_time
        global total_detail_time
        total_detail_time += detail_duration

        return detail

    def extract_download_links(self, doc, detail):
        # æå–é¡µé¢ä¸­æ‰€æœ‰æ–‡æœ¬å†…å®¹ï¼Œå¯»æ‰¾é“¾æ¥
        page_text = doc.get_text()

        for pan_type, regex in PAN_LINK_REGEXES.items():
            matches = regex.findall(page_text)
            for pan_link in matches:
                # æå–å¯†ç ï¼ˆå¦‚æœæœ‰ï¼‰
                password = self.extract_password_from_text(page_text, pan_link)
                self.add_download_link(detail, pan_type, pan_link, password)

        # 4. åœ¨ç‰¹å®šçš„ä¸‹è½½åŒºåŸŸæŸ¥æ‰¾é“¾æ¥
        for downlist_section in doc.select(".hl-rb-downlist"):
            # è·å–è´¨é‡ç‰ˆæœ¬ä¿¡æ¯
            current_quality = ""
            for tab_btn in downlist_section.select(".hl-tabs-btn"):
                if "active" in tab_btn.get("class", []):
                    current_quality = tab_btn.get_text(strip=True)

            # æå–å„ç§ä¸‹è½½é“¾æ¥
            for link_item in downlist_section.select(".hl-downs-list li"):
                item_text = link_item.get_text()
                item_html = str(link_item)

                # ä» data-clipboard-text å±æ€§æå–é“¾æ¥
                clipboard_text = link_item.select_one(".down-copy")
                if clipboard_text and "data-clipboard-text" in clipboard_text.attrs:
                    self.process_found_link(detail, clipboard_text["data-clipboard-text"], current_quality)

                # ä» href å±æ€§æå–é“¾æ¥
                for link in link_item.select("a"):
                    if "href" in link.attrs:
                        self.process_found_link(detail, link["href"], current_quality)

                # ä»æ–‡æœ¬å†…å®¹ä¸­æå–é“¾æ¥
                self.extract_links_from_text(detail, item_text, current_quality)
                self.extract_links_from_text(detail, item_html, current_quality)

        # 5. åœ¨æ’­æ”¾æºåŒºåŸŸä¹ŸæŸ¥æ‰¾é“¾æ¥
        for playlist_section in doc.select(".hl-rb-playlist"):
            section_text = playlist_section.get_text()
            section_html = str(playlist_section)
            self.extract_links_from_text(detail, section_text, "æ’­æ”¾æº")
            self.extract_links_from_text(detail, section_html, "æ’­æ”¾æº")

    def process_found_link(self, detail, link, quality):
        if not link:
            return

        # æ£€æŸ¥ç½‘ç›˜é“¾æ¥
        for pan_type, regex in PAN_LINK_REGEXES.items():
            if regex.match(link):
                password = self.extract_password_from_link(link)
                self.add_download_link(detail, pan_type, link, password)
                return

    def extract_links_from_text(self, detail, text, quality):
        # ç½‘ç›˜é“¾æ¥
        for pan_type, regex in PAN_LINK_REGEXES.items():
            matches = regex.findall(text)
            for pan_link in matches:
                password = self.extract_password_from_text(text, pan_link)
                self.add_download_link(detail, pan_type, pan_link, password)

    def extract_password_from_link(self, link):
        # é¦–å…ˆæ£€æŸ¥URLå‚æ•°ä¸­çš„å¯†ç 
        for regex in PASSWORD_REGEXES:
            matches = regex.search(link)
            if matches and len(matches.groups()) > 0:
                return matches.group(1)
        return ""

    def extract_password_from_text(self, text, link):
        # é¦–å…ˆä»é“¾æ¥æœ¬èº«æå–å¯†ç 
        password = self.extract_password_from_link(link)
        if password:
            return password

        # ç„¶åä»å‘¨å›´æ–‡æœ¬ä¸­æŸ¥æ‰¾å¯†ç 
        for regex in PASSWORD_REGEXES:
            matches = regex.search(text)
            if matches and len(matches.groups()) > 0:
                return matches.group(1)

        return ""

    def add_download_link(self, detail, link_type, link_url, password):
        if not link_url:
            return

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        for existing_link in detail['downloads']:
            if existing_link['url'] == link_url:
                return

        # åˆ›å»ºé“¾æ¥å¯¹è±¡
        link = {
            "type": link_type,
            "url": link_url,
            "password": password,
        }

        detail['downloads'].append(link)

    def do_request_with_retry(self, url, headers):
        max_retries = 3
        last_err = None

        if DEBUG_MODE:
            logging.debug(f"ğŸ”„ [Fox4k DEBUG] å¼€å§‹é‡è¯•æœºåˆ¶ - æœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries}")

        for i in range(max_retries):
            if DEBUG_MODE:
                logging.debug(f"ğŸ”„ [Fox4k DEBUG] ç¬¬ {i+1}/{max_retries} æ¬¡å°è¯•")

            if i > 0:
                # æŒ‡æ•°é€€é¿é‡è¯•
                backoff = (2 ** (i - 1)) * 0.2  # 0.2, 0.4, 0.8 seconds
                if DEBUG_MODE:
                    logging.debug(f"â³ [Fox4k DEBUG] ç­‰å¾… {backoff} ç§’åé‡è¯•")
                time.sleep(backoff)

            try:
                attempt_start = time.time()
                resp = self.optimized_client.get(url, headers=headers, timeout=DEFAULT_TIMEOUT)
                attempt_duration = time.time() - attempt_start

                if DEBUG_MODE:
                    logging.debug(f"ğŸ”§ [Fox4k DEBUG] ç¬¬ {i+1} æ¬¡å°è¯•è€—æ—¶: {attempt_duration}ç§’")

                if resp.status_code == 200:
                    if DEBUG_MODE:
                        logging.debug(f"âœ… [Fox4k DEBUG] ç¬¬ {i+1} æ¬¡å°è¯•æˆåŠŸ!")
                    return resp, None

                if DEBUG_MODE:
                    logging.debug(f"âŒ [Fox4k DEBUG] ç¬¬ {i+1} æ¬¡å°è¯•çŠ¶æ€ç å¼‚å¸¸: {resp.status_code}")

                # è¯»å–å“åº”ä½“ä»¥ä¾¿è°ƒè¯•
                if resp.text and len(resp.text) > 0:
                    body_preview = resp.text
                    if len(body_preview) > 200:
                        body_preview = body_preview[:200] + "..."
                    if DEBUG_MODE:
                        logging.debug(f"ğŸ”§ [Fox4k DEBUG] å“åº”ä½“é¢„è§ˆ: {body_preview}")

                last_err = f"çŠ¶æ€ç  {resp.status_code}"
            except Exception as e:
                if DEBUG_MODE:
                    logging.debug(f"âŒ [Fox4k DEBUG] ç¬¬ {i+1} æ¬¡å°è¯•å¤±è´¥: {e}")
                last_err = e
                continue

        if DEBUG_MODE:
            logging.debug("âŒ [Fox4k DEBUG] æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†!")
        return None, f"é‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥: {last_err}"


    def filter_results_by_keyword(self, results, keyword):
        # ç®€å•çš„å…³é”®è¯è¿‡æ»¤
        filtered_results = []
        for result in results:
            if keyword.lower() in result['title'].lower() or keyword.lower() in result['content'].lower():
                filtered_results.append(result)
        return filtered_results

    def async_search_with_result(self, keyword, search_func, *args):
        # ç®€å•çš„å¼‚æ­¥æœç´¢å®ç°
        try:
            results, err = search_func(keyword)
            return {"results": results or [], "is_final": True}, err
        except Exception as e:
            return {"results": [], "is_final": True}, str(e)